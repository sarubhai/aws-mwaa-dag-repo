# About aws-mwaa-dag-repo

This repository contains dags to be deployed to AWS MWAA (airflow) using Github Workflows. It also contains local development environment for dags using aws-mwaa-local-runner, which provides a command line interface (CLI) utility that replicates an MWAA environment locally.


## About the CLI

The CLI builds a Docker container image locally thatâ€™s similar to a MWAA production image. This allows you to run a local Apache Airflow environment to develop and test DAGs, custom plugins, and dependencies before deploying to MWAA.

## What this repo contains

```text
dags/
  custom_dag.py
  example_dag_with_taskflow_api.py    
  process_employees.py
  sql/
    employees_schema.sql
docker/
  config/
    airflow.cfg
    constraints.txt
    mwaa-base-providers-requirements.txt
    webserver_config.py
    .env.localrunner
  script/
    bootstrap.sh
    entrypoint.sh
    systemlibs.sh
    generate_key.sh
  docker-compose-local.yml
  docker-compose-resetdb.yml
  docker-compose-sequential.yml
  Dockerfile
plugins/
  hooks/
    cat_fact_hook.py
  operators/
    basic_math_operator.py
  sensors/
    my_airflow_sensor.py
  dag-timezone-plugin.py
requirements/  
  requirements.txt
startup_script/
  startup.sh
mwaa-local-env
```

## Prerequisites

- **macOS**: [Install Docker Desktop](https://docs.docker.com/desktop/).

## Get started

### Step one: Building the Docker image

Build the Docker container image using the following command:

```bash
./mwaa-local-env build-image
```

**Note**: it takes several minutes to build the Docker image locally.

### Step two: Running Apache Airflow

#### Local runner

Runs a local Apache Airflow environment that is a close representation of MWAA by configuration.

```bash
./mwaa-local-env start
```

To stop the local environment, Ctrl+C on the terminal and wait till the local runner and the postgres containers are stopped.

### Step three: Accessing the Airflow UI

By default, the `bootstrap.sh` script creates a username and password for your local Airflow environment.

- Username: `admin`
- Password: `test`

#### Airflow UI

- Open the Apache Airlfow UI: <http://localhost:8080/>.


## What's next?

- Learn how to upload the requirements.txt file to your Amazon S3 bucket in [Installing Python dependencies](https://docs.aws.amazon.com/mwaa/latest/userguide/working-dags-dependencies.html).
- Learn how to upload the DAG code to the dags folder in your Amazon S3 bucket in [Adding or updating DAGs](https://docs.aws.amazon.com/mwaa/latest/userguide/configuring-dag-folder.html).
- Learn more about how to upload the plugins.zip file to your Amazon S3 bucket in [Installing custom plugins](https://docs.aws.amazon.com/mwaa/latest/userguide/configuring-dag-import-plugins.html).
